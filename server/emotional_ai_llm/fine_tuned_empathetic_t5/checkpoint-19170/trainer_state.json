{
  "best_global_step": 19170,
  "best_metric": 2.409820318222046,
  "best_model_checkpoint": "./fine_tuned_empathetic_t5\\checkpoint-19170",
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 19170,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.010432968179447054,
      "grad_norm": 1.8295807838439941,
      "learning_rate": 1.993114241001565e-05,
      "loss": 2.9473,
      "step": 100
    },
    {
      "epoch": 0.020865936358894107,
      "grad_norm": 2.0004825592041016,
      "learning_rate": 1.986158928881934e-05,
      "loss": 2.7394,
      "step": 200
    },
    {
      "epoch": 0.03129890453834116,
      "grad_norm": 1.6302393674850464,
      "learning_rate": 1.9792036167623024e-05,
      "loss": 2.6618,
      "step": 300
    },
    {
      "epoch": 0.041731872717788214,
      "grad_norm": 3.237431049346924,
      "learning_rate": 1.972248304642671e-05,
      "loss": 2.5933,
      "step": 400
    },
    {
      "epoch": 0.05216484089723526,
      "grad_norm": 1.7434358596801758,
      "learning_rate": 1.9652929925230398e-05,
      "loss": 2.4866,
      "step": 500
    },
    {
      "epoch": 0.06259780907668232,
      "grad_norm": 1.701237678527832,
      "learning_rate": 1.9583376804034083e-05,
      "loss": 2.491,
      "step": 600
    },
    {
      "epoch": 0.07303077725612937,
      "grad_norm": 1.9183669090270996,
      "learning_rate": 1.9513823682837768e-05,
      "loss": 2.4375,
      "step": 700
    },
    {
      "epoch": 0.08346374543557643,
      "grad_norm": 1.4619492292404175,
      "learning_rate": 1.9444270561641456e-05,
      "loss": 2.4867,
      "step": 800
    },
    {
      "epoch": 0.09389671361502347,
      "grad_norm": 1.9214144945144653,
      "learning_rate": 1.937471744044514e-05,
      "loss": 2.4282,
      "step": 900
    },
    {
      "epoch": 0.10432968179447052,
      "grad_norm": 1.9464659690856934,
      "learning_rate": 1.930516431924883e-05,
      "loss": 2.4425,
      "step": 1000
    },
    {
      "epoch": 0.11476264997391758,
      "grad_norm": 1.2720372676849365,
      "learning_rate": 1.9235611198052515e-05,
      "loss": 2.3614,
      "step": 1100
    },
    {
      "epoch": 0.12519561815336464,
      "grad_norm": 1.4946857690811157,
      "learning_rate": 1.91660580768562e-05,
      "loss": 2.4309,
      "step": 1200
    },
    {
      "epoch": 0.1356285863328117,
      "grad_norm": 2.239875078201294,
      "learning_rate": 1.9096504955659888e-05,
      "loss": 2.4477,
      "step": 1300
    },
    {
      "epoch": 0.14606155451225875,
      "grad_norm": 2.1330230236053467,
      "learning_rate": 1.9026951834463573e-05,
      "loss": 2.4274,
      "step": 1400
    },
    {
      "epoch": 0.1564945226917058,
      "grad_norm": 2.3048934936523438,
      "learning_rate": 1.8957398713267258e-05,
      "loss": 2.4111,
      "step": 1500
    },
    {
      "epoch": 0.16692749087115286,
      "grad_norm": 1.8368514776229858,
      "learning_rate": 1.8887845592070947e-05,
      "loss": 2.3159,
      "step": 1600
    },
    {
      "epoch": 0.17736045905059988,
      "grad_norm": 1.7324750423431396,
      "learning_rate": 1.8818292470874632e-05,
      "loss": 2.3837,
      "step": 1700
    },
    {
      "epoch": 0.18779342723004694,
      "grad_norm": 1.7323551177978516,
      "learning_rate": 1.8748739349678317e-05,
      "loss": 2.3619,
      "step": 1800
    },
    {
      "epoch": 0.198226395409494,
      "grad_norm": 1.4003400802612305,
      "learning_rate": 1.8679186228482005e-05,
      "loss": 2.3664,
      "step": 1900
    },
    {
      "epoch": 0.20865936358894105,
      "grad_norm": 1.8846561908721924,
      "learning_rate": 1.860963310728569e-05,
      "loss": 2.3544,
      "step": 2000
    },
    {
      "epoch": 0.2190923317683881,
      "grad_norm": 1.424579381942749,
      "learning_rate": 1.8540079986089375e-05,
      "loss": 2.343,
      "step": 2100
    },
    {
      "epoch": 0.22952529994783516,
      "grad_norm": 1.3999730348587036,
      "learning_rate": 1.8470526864893064e-05,
      "loss": 2.3642,
      "step": 2200
    },
    {
      "epoch": 0.23995826812728221,
      "grad_norm": 2.0190975666046143,
      "learning_rate": 1.8400973743696752e-05,
      "loss": 2.3727,
      "step": 2300
    },
    {
      "epoch": 0.25039123630672927,
      "grad_norm": 1.6625912189483643,
      "learning_rate": 1.8331420622500434e-05,
      "loss": 2.3139,
      "step": 2400
    },
    {
      "epoch": 0.2608242044861763,
      "grad_norm": 2.1053714752197266,
      "learning_rate": 1.8261867501304122e-05,
      "loss": 2.3549,
      "step": 2500
    },
    {
      "epoch": 0.2712571726656234,
      "grad_norm": 1.6632411479949951,
      "learning_rate": 1.819231438010781e-05,
      "loss": 2.3411,
      "step": 2600
    },
    {
      "epoch": 0.28169014084507044,
      "grad_norm": 1.5141505002975464,
      "learning_rate": 1.8122761258911496e-05,
      "loss": 2.3473,
      "step": 2700
    },
    {
      "epoch": 0.2921231090245175,
      "grad_norm": 1.6953282356262207,
      "learning_rate": 1.805320813771518e-05,
      "loss": 2.3068,
      "step": 2800
    },
    {
      "epoch": 0.30255607720396455,
      "grad_norm": 2.0351288318634033,
      "learning_rate": 1.798365501651887e-05,
      "loss": 2.3012,
      "step": 2900
    },
    {
      "epoch": 0.3129890453834116,
      "grad_norm": 1.6801096200942993,
      "learning_rate": 1.7914101895322554e-05,
      "loss": 2.3307,
      "step": 3000
    },
    {
      "epoch": 0.32342201356285866,
      "grad_norm": 2.278379440307617,
      "learning_rate": 1.784454877412624e-05,
      "loss": 2.2986,
      "step": 3100
    },
    {
      "epoch": 0.3338549817423057,
      "grad_norm": 1.7483782768249512,
      "learning_rate": 1.7774995652929928e-05,
      "loss": 2.3189,
      "step": 3200
    },
    {
      "epoch": 0.3442879499217527,
      "grad_norm": 1.5128766298294067,
      "learning_rate": 1.7705442531733613e-05,
      "loss": 2.2935,
      "step": 3300
    },
    {
      "epoch": 0.35472091810119977,
      "grad_norm": 1.4195107221603394,
      "learning_rate": 1.7635889410537298e-05,
      "loss": 2.2922,
      "step": 3400
    },
    {
      "epoch": 0.3651538862806468,
      "grad_norm": 1.6019829511642456,
      "learning_rate": 1.7566336289340986e-05,
      "loss": 2.2668,
      "step": 3500
    },
    {
      "epoch": 0.3755868544600939,
      "grad_norm": 1.219707727432251,
      "learning_rate": 1.749678316814467e-05,
      "loss": 2.2899,
      "step": 3600
    },
    {
      "epoch": 0.38601982263954093,
      "grad_norm": 1.7118070125579834,
      "learning_rate": 1.7427230046948356e-05,
      "loss": 2.2503,
      "step": 3700
    },
    {
      "epoch": 0.396452790818988,
      "grad_norm": 1.6371277570724487,
      "learning_rate": 1.7357676925752045e-05,
      "loss": 2.2679,
      "step": 3800
    },
    {
      "epoch": 0.40688575899843504,
      "grad_norm": 1.427085518836975,
      "learning_rate": 1.728812380455573e-05,
      "loss": 2.2461,
      "step": 3900
    },
    {
      "epoch": 0.4173187271778821,
      "grad_norm": 1.6259034872055054,
      "learning_rate": 1.721857068335942e-05,
      "loss": 2.2808,
      "step": 4000
    },
    {
      "epoch": 0.42775169535732915,
      "grad_norm": 1.6913827657699585,
      "learning_rate": 1.7149017562163103e-05,
      "loss": 2.263,
      "step": 4100
    },
    {
      "epoch": 0.4381846635367762,
      "grad_norm": 1.794116497039795,
      "learning_rate": 1.707946444096679e-05,
      "loss": 2.3224,
      "step": 4200
    },
    {
      "epoch": 0.44861763171622326,
      "grad_norm": 1.9418957233428955,
      "learning_rate": 1.7009911319770477e-05,
      "loss": 2.2594,
      "step": 4300
    },
    {
      "epoch": 0.4590505998956703,
      "grad_norm": 2.4830329418182373,
      "learning_rate": 1.6940358198574162e-05,
      "loss": 2.3222,
      "step": 4400
    },
    {
      "epoch": 0.4694835680751174,
      "grad_norm": 1.8814245462417603,
      "learning_rate": 1.687080507737785e-05,
      "loss": 2.3063,
      "step": 4500
    },
    {
      "epoch": 0.47991653625456443,
      "grad_norm": 1.649562120437622,
      "learning_rate": 1.6801251956181535e-05,
      "loss": 2.2561,
      "step": 4600
    },
    {
      "epoch": 0.4903495044340115,
      "grad_norm": 2.2628159523010254,
      "learning_rate": 1.673169883498522e-05,
      "loss": 2.3168,
      "step": 4700
    },
    {
      "epoch": 0.5007824726134585,
      "grad_norm": 1.8795124292373657,
      "learning_rate": 1.666214571378891e-05,
      "loss": 2.218,
      "step": 4800
    },
    {
      "epoch": 0.5112154407929056,
      "grad_norm": 1.2191436290740967,
      "learning_rate": 1.6592592592592594e-05,
      "loss": 2.304,
      "step": 4900
    },
    {
      "epoch": 0.5216484089723527,
      "grad_norm": 2.2339184284210205,
      "learning_rate": 1.652303947139628e-05,
      "loss": 2.2936,
      "step": 5000
    },
    {
      "epoch": 0.5320813771517997,
      "grad_norm": 17.614450454711914,
      "learning_rate": 1.6453486350199967e-05,
      "loss": 2.2441,
      "step": 5100
    },
    {
      "epoch": 0.5425143453312468,
      "grad_norm": 1.7166777849197388,
      "learning_rate": 1.6383933229003653e-05,
      "loss": 2.229,
      "step": 5200
    },
    {
      "epoch": 0.5529473135106938,
      "grad_norm": 1.7943191528320312,
      "learning_rate": 1.631438010780734e-05,
      "loss": 2.3296,
      "step": 5300
    },
    {
      "epoch": 0.5633802816901409,
      "grad_norm": 2.071654796600342,
      "learning_rate": 1.6244826986611026e-05,
      "loss": 2.2517,
      "step": 5400
    },
    {
      "epoch": 0.5738132498695879,
      "grad_norm": 1.774187684059143,
      "learning_rate": 1.617527386541471e-05,
      "loss": 2.22,
      "step": 5500
    },
    {
      "epoch": 0.584246218049035,
      "grad_norm": 1.6430623531341553,
      "learning_rate": 1.61057207442184e-05,
      "loss": 2.294,
      "step": 5600
    },
    {
      "epoch": 0.594679186228482,
      "grad_norm": 1.5411663055419922,
      "learning_rate": 1.6036167623022085e-05,
      "loss": 2.2661,
      "step": 5700
    },
    {
      "epoch": 0.6051121544079291,
      "grad_norm": 1.891314148902893,
      "learning_rate": 1.596661450182577e-05,
      "loss": 2.269,
      "step": 5800
    },
    {
      "epoch": 0.6155451225873761,
      "grad_norm": 1.6631615161895752,
      "learning_rate": 1.5897061380629458e-05,
      "loss": 2.2163,
      "step": 5900
    },
    {
      "epoch": 0.6259780907668232,
      "grad_norm": 2.085470676422119,
      "learning_rate": 1.5827508259433143e-05,
      "loss": 2.2941,
      "step": 6000
    },
    {
      "epoch": 0.6364110589462703,
      "grad_norm": 1.742026686668396,
      "learning_rate": 1.5757955138236828e-05,
      "loss": 2.2021,
      "step": 6100
    },
    {
      "epoch": 0.6468440271257173,
      "grad_norm": 1.3230868577957153,
      "learning_rate": 1.5688402017040517e-05,
      "loss": 2.306,
      "step": 6200
    },
    {
      "epoch": 0.6572769953051644,
      "grad_norm": 1.6285609006881714,
      "learning_rate": 1.56188488958442e-05,
      "loss": 2.281,
      "step": 6300
    },
    {
      "epoch": 0.6677099634846114,
      "grad_norm": 1.86021888256073,
      "learning_rate": 1.5549295774647887e-05,
      "loss": 2.2086,
      "step": 6400
    },
    {
      "epoch": 0.6781429316640585,
      "grad_norm": 1.623491644859314,
      "learning_rate": 1.5479742653451575e-05,
      "loss": 2.2217,
      "step": 6500
    },
    {
      "epoch": 0.6885758998435054,
      "grad_norm": 1.3431731462478638,
      "learning_rate": 1.5410189532255264e-05,
      "loss": 2.2666,
      "step": 6600
    },
    {
      "epoch": 0.6990088680229525,
      "grad_norm": 1.5783677101135254,
      "learning_rate": 1.5340636411058945e-05,
      "loss": 2.2172,
      "step": 6700
    },
    {
      "epoch": 0.7094418362023995,
      "grad_norm": 2.588593006134033,
      "learning_rate": 1.5271083289862634e-05,
      "loss": 2.1607,
      "step": 6800
    },
    {
      "epoch": 0.7198748043818466,
      "grad_norm": 1.3972066640853882,
      "learning_rate": 1.520153016866632e-05,
      "loss": 2.2949,
      "step": 6900
    },
    {
      "epoch": 0.7303077725612936,
      "grad_norm": 1.4020544290542603,
      "learning_rate": 1.5131977047470005e-05,
      "loss": 2.2648,
      "step": 7000
    },
    {
      "epoch": 0.7407407407407407,
      "grad_norm": 1.4870984554290771,
      "learning_rate": 1.5062423926273692e-05,
      "loss": 2.2681,
      "step": 7100
    },
    {
      "epoch": 0.7511737089201878,
      "grad_norm": 2.097010612487793,
      "learning_rate": 1.4992870805077379e-05,
      "loss": 2.2381,
      "step": 7200
    },
    {
      "epoch": 0.7616066770996348,
      "grad_norm": 1.6063036918640137,
      "learning_rate": 1.4923317683881064e-05,
      "loss": 2.2378,
      "step": 7300
    },
    {
      "epoch": 0.7720396452790819,
      "grad_norm": 3.2855207920074463,
      "learning_rate": 1.485376456268475e-05,
      "loss": 2.1757,
      "step": 7400
    },
    {
      "epoch": 0.7824726134585289,
      "grad_norm": 1.552617073059082,
      "learning_rate": 1.478421144148844e-05,
      "loss": 2.2146,
      "step": 7500
    },
    {
      "epoch": 0.792905581637976,
      "grad_norm": 1.599290132522583,
      "learning_rate": 1.4714658320292124e-05,
      "loss": 2.26,
      "step": 7600
    },
    {
      "epoch": 0.803338549817423,
      "grad_norm": 1.9535415172576904,
      "learning_rate": 1.4645105199095811e-05,
      "loss": 2.2118,
      "step": 7700
    },
    {
      "epoch": 0.8137715179968701,
      "grad_norm": 1.3708364963531494,
      "learning_rate": 1.4575552077899498e-05,
      "loss": 2.222,
      "step": 7800
    },
    {
      "epoch": 0.8242044861763171,
      "grad_norm": 1.8914967775344849,
      "learning_rate": 1.4505998956703183e-05,
      "loss": 2.2265,
      "step": 7900
    },
    {
      "epoch": 0.8346374543557642,
      "grad_norm": 1.6684082746505737,
      "learning_rate": 1.443644583550687e-05,
      "loss": 2.2123,
      "step": 8000
    },
    {
      "epoch": 0.8450704225352113,
      "grad_norm": 1.9781007766723633,
      "learning_rate": 1.4366892714310556e-05,
      "loss": 2.211,
      "step": 8100
    },
    {
      "epoch": 0.8555033907146583,
      "grad_norm": 1.55623197555542,
      "learning_rate": 1.4297339593114241e-05,
      "loss": 2.1993,
      "step": 8200
    },
    {
      "epoch": 0.8659363588941054,
      "grad_norm": 1.7076084613800049,
      "learning_rate": 1.4227786471917928e-05,
      "loss": 2.1887,
      "step": 8300
    },
    {
      "epoch": 0.8763693270735524,
      "grad_norm": 1.2499266862869263,
      "learning_rate": 1.4158233350721615e-05,
      "loss": 2.2592,
      "step": 8400
    },
    {
      "epoch": 0.8868022952529995,
      "grad_norm": 3.1231026649475098,
      "learning_rate": 1.40886802295253e-05,
      "loss": 2.241,
      "step": 8500
    },
    {
      "epoch": 0.8972352634324465,
      "grad_norm": 1.423039436340332,
      "learning_rate": 1.4019127108328987e-05,
      "loss": 2.2878,
      "step": 8600
    },
    {
      "epoch": 0.9076682316118936,
      "grad_norm": 1.438254475593567,
      "learning_rate": 1.3949573987132673e-05,
      "loss": 2.2673,
      "step": 8700
    },
    {
      "epoch": 0.9181011997913406,
      "grad_norm": 1.837646484375,
      "learning_rate": 1.3880020865936362e-05,
      "loss": 2.2241,
      "step": 8800
    },
    {
      "epoch": 0.9285341679707877,
      "grad_norm": 2.080115795135498,
      "learning_rate": 1.3810467744740045e-05,
      "loss": 2.2166,
      "step": 8900
    },
    {
      "epoch": 0.9389671361502347,
      "grad_norm": 1.4019520282745361,
      "learning_rate": 1.3740914623543734e-05,
      "loss": 2.2382,
      "step": 9000
    },
    {
      "epoch": 0.9494001043296818,
      "grad_norm": 1.488574743270874,
      "learning_rate": 1.367136150234742e-05,
      "loss": 2.1613,
      "step": 9100
    },
    {
      "epoch": 0.9598330725091289,
      "grad_norm": 1.3873180150985718,
      "learning_rate": 1.3601808381151105e-05,
      "loss": 2.1482,
      "step": 9200
    },
    {
      "epoch": 0.9702660406885759,
      "grad_norm": 1.4588485956192017,
      "learning_rate": 1.3532255259954792e-05,
      "loss": 2.2174,
      "step": 9300
    },
    {
      "epoch": 0.980699008868023,
      "grad_norm": 1.4075289964675903,
      "learning_rate": 1.3462702138758479e-05,
      "loss": 2.2303,
      "step": 9400
    },
    {
      "epoch": 0.99113197704747,
      "grad_norm": 1.8619922399520874,
      "learning_rate": 1.3393149017562164e-05,
      "loss": 2.2189,
      "step": 9500
    },
    {
      "epoch": 1.0,
      "eval_loss": 2.4303009510040283,
      "eval_runtime": 599.3232,
      "eval_samples_per_second": 20.073,
      "eval_steps_per_second": 2.509,
      "step": 9585
    },
    {
      "epoch": 1.001564945226917,
      "grad_norm": 1.9372514486312866,
      "learning_rate": 1.332359589636585e-05,
      "loss": 2.2286,
      "step": 9600
    },
    {
      "epoch": 1.0119979134063641,
      "grad_norm": 1.7740973234176636,
      "learning_rate": 1.3254042775169537e-05,
      "loss": 2.1786,
      "step": 9700
    },
    {
      "epoch": 1.0224308815858112,
      "grad_norm": 1.3921247720718384,
      "learning_rate": 1.3184489653973222e-05,
      "loss": 2.1186,
      "step": 9800
    },
    {
      "epoch": 1.0328638497652582,
      "grad_norm": 1.5631380081176758,
      "learning_rate": 1.311493653277691e-05,
      "loss": 2.2016,
      "step": 9900
    },
    {
      "epoch": 1.0432968179447053,
      "grad_norm": 2.6643247604370117,
      "learning_rate": 1.3045383411580596e-05,
      "loss": 2.1595,
      "step": 10000
    },
    {
      "epoch": 1.0537297861241524,
      "grad_norm": 1.3745474815368652,
      "learning_rate": 1.2975830290384281e-05,
      "loss": 2.1867,
      "step": 10100
    },
    {
      "epoch": 1.0641627543035994,
      "grad_norm": 2.403988838195801,
      "learning_rate": 1.2906277169187968e-05,
      "loss": 2.1474,
      "step": 10200
    },
    {
      "epoch": 1.0745957224830465,
      "grad_norm": 2.4486138820648193,
      "learning_rate": 1.2836724047991656e-05,
      "loss": 2.2426,
      "step": 10300
    },
    {
      "epoch": 1.0850286906624935,
      "grad_norm": 1.2739351987838745,
      "learning_rate": 1.276717092679534e-05,
      "loss": 2.2544,
      "step": 10400
    },
    {
      "epoch": 1.0954616588419406,
      "grad_norm": 1.3538988828659058,
      "learning_rate": 1.2697617805599028e-05,
      "loss": 2.1895,
      "step": 10500
    },
    {
      "epoch": 1.1058946270213876,
      "grad_norm": 1.4747755527496338,
      "learning_rate": 1.2628064684402715e-05,
      "loss": 2.1522,
      "step": 10600
    },
    {
      "epoch": 1.1163275952008347,
      "grad_norm": 1.5045117139816284,
      "learning_rate": 1.25585115632064e-05,
      "loss": 2.1984,
      "step": 10700
    },
    {
      "epoch": 1.1267605633802817,
      "grad_norm": 1.3936480283737183,
      "learning_rate": 1.2488958442010086e-05,
      "loss": 2.2277,
      "step": 10800
    },
    {
      "epoch": 1.1371935315597288,
      "grad_norm": 2.5212862491607666,
      "learning_rate": 1.2419405320813773e-05,
      "loss": 2.1223,
      "step": 10900
    },
    {
      "epoch": 1.1476264997391759,
      "grad_norm": 1.6709803342819214,
      "learning_rate": 1.2349852199617458e-05,
      "loss": 2.1717,
      "step": 11000
    },
    {
      "epoch": 1.158059467918623,
      "grad_norm": 1.9366331100463867,
      "learning_rate": 1.2280299078421145e-05,
      "loss": 2.1875,
      "step": 11100
    },
    {
      "epoch": 1.16849243609807,
      "grad_norm": 1.6861389875411987,
      "learning_rate": 1.2210745957224832e-05,
      "loss": 2.2101,
      "step": 11200
    },
    {
      "epoch": 1.178925404277517,
      "grad_norm": 1.9068249464035034,
      "learning_rate": 1.2141192836028517e-05,
      "loss": 2.1351,
      "step": 11300
    },
    {
      "epoch": 1.189358372456964,
      "grad_norm": 1.2848622798919678,
      "learning_rate": 1.2071639714832204e-05,
      "loss": 2.1476,
      "step": 11400
    },
    {
      "epoch": 1.1997913406364111,
      "grad_norm": 1.8734493255615234,
      "learning_rate": 1.200208659363589e-05,
      "loss": 2.2174,
      "step": 11500
    },
    {
      "epoch": 1.2102243088158582,
      "grad_norm": 1.7779080867767334,
      "learning_rate": 1.1932533472439575e-05,
      "loss": 2.1616,
      "step": 11600
    },
    {
      "epoch": 1.2206572769953052,
      "grad_norm": 1.562444806098938,
      "learning_rate": 1.1862980351243262e-05,
      "loss": 2.2102,
      "step": 11700
    },
    {
      "epoch": 1.2310902451747523,
      "grad_norm": 1.4516748189926147,
      "learning_rate": 1.179342723004695e-05,
      "loss": 2.2368,
      "step": 11800
    },
    {
      "epoch": 1.2415232133541994,
      "grad_norm": 1.6658493280410767,
      "learning_rate": 1.1723874108850634e-05,
      "loss": 2.145,
      "step": 11900
    },
    {
      "epoch": 1.2519561815336462,
      "grad_norm": 1.3551759719848633,
      "learning_rate": 1.1654320987654322e-05,
      "loss": 2.1934,
      "step": 12000
    },
    {
      "epoch": 1.2623891497130932,
      "grad_norm": 1.3096683025360107,
      "learning_rate": 1.1584767866458009e-05,
      "loss": 2.1851,
      "step": 12100
    },
    {
      "epoch": 1.2728221178925403,
      "grad_norm": 1.647213101387024,
      "learning_rate": 1.1515214745261694e-05,
      "loss": 2.2296,
      "step": 12200
    },
    {
      "epoch": 1.2832550860719873,
      "grad_norm": 1.6233997344970703,
      "learning_rate": 1.1445661624065381e-05,
      "loss": 2.1871,
      "step": 12300
    },
    {
      "epoch": 1.2936880542514344,
      "grad_norm": 1.4086689949035645,
      "learning_rate": 1.1376108502869068e-05,
      "loss": 2.1997,
      "step": 12400
    },
    {
      "epoch": 1.3041210224308815,
      "grad_norm": 1.6470075845718384,
      "learning_rate": 1.1306555381672753e-05,
      "loss": 2.1614,
      "step": 12500
    },
    {
      "epoch": 1.3145539906103285,
      "grad_norm": 1.3774412870407104,
      "learning_rate": 1.123700226047644e-05,
      "loss": 2.1973,
      "step": 12600
    },
    {
      "epoch": 1.3249869587897756,
      "grad_norm": 1.4038059711456299,
      "learning_rate": 1.1167449139280126e-05,
      "loss": 2.1338,
      "step": 12700
    },
    {
      "epoch": 1.3354199269692226,
      "grad_norm": 3.6316752433776855,
      "learning_rate": 1.1097896018083811e-05,
      "loss": 2.1607,
      "step": 12800
    },
    {
      "epoch": 1.3458528951486697,
      "grad_norm": 1.8366178274154663,
      "learning_rate": 1.1028342896887498e-05,
      "loss": 2.1688,
      "step": 12900
    },
    {
      "epoch": 1.3562858633281167,
      "grad_norm": 1.1508066654205322,
      "learning_rate": 1.0958789775691185e-05,
      "loss": 2.1202,
      "step": 13000
    },
    {
      "epoch": 1.3667188315075638,
      "grad_norm": 2.3931989669799805,
      "learning_rate": 1.0889236654494873e-05,
      "loss": 2.1773,
      "step": 13100
    },
    {
      "epoch": 1.3771517996870108,
      "grad_norm": 1.570212721824646,
      "learning_rate": 1.0819683533298556e-05,
      "loss": 2.1033,
      "step": 13200
    },
    {
      "epoch": 1.387584767866458,
      "grad_norm": 1.5685160160064697,
      "learning_rate": 1.0750130412102245e-05,
      "loss": 2.1101,
      "step": 13300
    },
    {
      "epoch": 1.398017736045905,
      "grad_norm": 2.1709954738616943,
      "learning_rate": 1.0680577290905932e-05,
      "loss": 2.1075,
      "step": 13400
    },
    {
      "epoch": 1.408450704225352,
      "grad_norm": 1.4779229164123535,
      "learning_rate": 1.0611024169709617e-05,
      "loss": 2.1877,
      "step": 13500
    },
    {
      "epoch": 1.418883672404799,
      "grad_norm": 1.690137267112732,
      "learning_rate": 1.0541471048513303e-05,
      "loss": 2.1932,
      "step": 13600
    },
    {
      "epoch": 1.4293166405842461,
      "grad_norm": 1.5191096067428589,
      "learning_rate": 1.047191792731699e-05,
      "loss": 2.1083,
      "step": 13700
    },
    {
      "epoch": 1.4397496087636932,
      "grad_norm": 1.6578333377838135,
      "learning_rate": 1.0402364806120675e-05,
      "loss": 2.1386,
      "step": 13800
    },
    {
      "epoch": 1.4501825769431402,
      "grad_norm": 1.9363734722137451,
      "learning_rate": 1.0332811684924362e-05,
      "loss": 2.1456,
      "step": 13900
    },
    {
      "epoch": 1.4606155451225873,
      "grad_norm": 1.6245981454849243,
      "learning_rate": 1.0263258563728049e-05,
      "loss": 2.2569,
      "step": 14000
    },
    {
      "epoch": 1.4710485133020343,
      "grad_norm": 1.790504813194275,
      "learning_rate": 1.0193705442531734e-05,
      "loss": 2.1877,
      "step": 14100
    },
    {
      "epoch": 1.4814814814814814,
      "grad_norm": 2.1502392292022705,
      "learning_rate": 1.012415232133542e-05,
      "loss": 2.1438,
      "step": 14200
    },
    {
      "epoch": 1.4919144496609285,
      "grad_norm": 1.2168112993240356,
      "learning_rate": 1.0054599200139107e-05,
      "loss": 2.2138,
      "step": 14300
    },
    {
      "epoch": 1.5023474178403755,
      "grad_norm": 2.0165750980377197,
      "learning_rate": 9.985046078942794e-06,
      "loss": 2.161,
      "step": 14400
    },
    {
      "epoch": 1.5127803860198226,
      "grad_norm": 1.3330007791519165,
      "learning_rate": 9.915492957746479e-06,
      "loss": 2.1208,
      "step": 14500
    },
    {
      "epoch": 1.5232133541992696,
      "grad_norm": 1.2970781326293945,
      "learning_rate": 9.845939836550166e-06,
      "loss": 2.2039,
      "step": 14600
    },
    {
      "epoch": 1.5336463223787167,
      "grad_norm": 1.319154143333435,
      "learning_rate": 9.776386715353853e-06,
      "loss": 2.1484,
      "step": 14700
    },
    {
      "epoch": 1.5440792905581637,
      "grad_norm": 1.788858413696289,
      "learning_rate": 9.70683359415754e-06,
      "loss": 2.1999,
      "step": 14800
    },
    {
      "epoch": 1.5545122587376108,
      "grad_norm": 1.3245588541030884,
      "learning_rate": 9.637280472961224e-06,
      "loss": 2.225,
      "step": 14900
    },
    {
      "epoch": 1.5649452269170578,
      "grad_norm": 1.7174296379089355,
      "learning_rate": 9.567727351764911e-06,
      "loss": 2.1241,
      "step": 15000
    },
    {
      "epoch": 1.575378195096505,
      "grad_norm": 1.5241174697875977,
      "learning_rate": 9.498174230568598e-06,
      "loss": 2.1819,
      "step": 15100
    },
    {
      "epoch": 1.585811163275952,
      "grad_norm": 1.6243852376937866,
      "learning_rate": 9.428621109372283e-06,
      "loss": 2.1482,
      "step": 15200
    },
    {
      "epoch": 1.596244131455399,
      "grad_norm": 1.6582274436950684,
      "learning_rate": 9.359067988175971e-06,
      "loss": 2.2513,
      "step": 15300
    },
    {
      "epoch": 1.606677099634846,
      "grad_norm": 1.191738486289978,
      "learning_rate": 9.289514866979656e-06,
      "loss": 2.1701,
      "step": 15400
    },
    {
      "epoch": 1.6171100678142931,
      "grad_norm": 1.2849183082580566,
      "learning_rate": 9.219961745783343e-06,
      "loss": 2.1889,
      "step": 15500
    },
    {
      "epoch": 1.6275430359937402,
      "grad_norm": 1.4168506860733032,
      "learning_rate": 9.15040862458703e-06,
      "loss": 2.1908,
      "step": 15600
    },
    {
      "epoch": 1.6379760041731872,
      "grad_norm": 1.539323329925537,
      "learning_rate": 9.080855503390715e-06,
      "loss": 2.1468,
      "step": 15700
    },
    {
      "epoch": 1.6484089723526343,
      "grad_norm": 1.7916358709335327,
      "learning_rate": 9.011302382194402e-06,
      "loss": 2.161,
      "step": 15800
    },
    {
      "epoch": 1.6588419405320813,
      "grad_norm": 1.3663049936294556,
      "learning_rate": 8.941749260998088e-06,
      "loss": 2.1679,
      "step": 15900
    },
    {
      "epoch": 1.6692749087115284,
      "grad_norm": 1.4948663711547852,
      "learning_rate": 8.872196139801773e-06,
      "loss": 2.1826,
      "step": 16000
    },
    {
      "epoch": 1.6797078768909754,
      "grad_norm": 1.086252212524414,
      "learning_rate": 8.80264301860546e-06,
      "loss": 2.2097,
      "step": 16100
    },
    {
      "epoch": 1.6901408450704225,
      "grad_norm": 1.606819748878479,
      "learning_rate": 8.733089897409147e-06,
      "loss": 2.1453,
      "step": 16200
    },
    {
      "epoch": 1.7005738132498696,
      "grad_norm": 1.5790084600448608,
      "learning_rate": 8.663536776212834e-06,
      "loss": 2.2064,
      "step": 16300
    },
    {
      "epoch": 1.7110067814293166,
      "grad_norm": 2.092726707458496,
      "learning_rate": 8.59398365501652e-06,
      "loss": 2.1934,
      "step": 16400
    },
    {
      "epoch": 1.7214397496087637,
      "grad_norm": 1.9644094705581665,
      "learning_rate": 8.524430533820205e-06,
      "loss": 2.1588,
      "step": 16500
    },
    {
      "epoch": 1.7318727177882107,
      "grad_norm": 1.4586458206176758,
      "learning_rate": 8.454877412623892e-06,
      "loss": 2.1728,
      "step": 16600
    },
    {
      "epoch": 1.7423056859676578,
      "grad_norm": 1.3836830854415894,
      "learning_rate": 8.385324291427579e-06,
      "loss": 2.1461,
      "step": 16700
    },
    {
      "epoch": 1.7527386541471048,
      "grad_norm": 1.6631091833114624,
      "learning_rate": 8.315771170231266e-06,
      "loss": 2.1369,
      "step": 16800
    },
    {
      "epoch": 1.763171622326552,
      "grad_norm": 2.1090757846832275,
      "learning_rate": 8.24621804903495e-06,
      "loss": 2.1991,
      "step": 16900
    },
    {
      "epoch": 1.773604590505999,
      "grad_norm": 1.613675594329834,
      "learning_rate": 8.176664927838638e-06,
      "loss": 2.2181,
      "step": 17000
    },
    {
      "epoch": 1.784037558685446,
      "grad_norm": 1.4058023691177368,
      "learning_rate": 8.107111806642324e-06,
      "loss": 2.1776,
      "step": 17100
    },
    {
      "epoch": 1.794470526864893,
      "grad_norm": 1.7817965745925903,
      "learning_rate": 8.03755868544601e-06,
      "loss": 2.1758,
      "step": 17200
    },
    {
      "epoch": 1.8049034950443401,
      "grad_norm": 2.3264689445495605,
      "learning_rate": 7.968005564249696e-06,
      "loss": 2.1632,
      "step": 17300
    },
    {
      "epoch": 1.8153364632237872,
      "grad_norm": 2.6228373050689697,
      "learning_rate": 7.898452443053383e-06,
      "loss": 2.1758,
      "step": 17400
    },
    {
      "epoch": 1.8257694314032342,
      "grad_norm": 1.8351163864135742,
      "learning_rate": 7.828899321857068e-06,
      "loss": 2.2038,
      "step": 17500
    },
    {
      "epoch": 1.8362023995826813,
      "grad_norm": 1.5009597539901733,
      "learning_rate": 7.759346200660756e-06,
      "loss": 2.1572,
      "step": 17600
    },
    {
      "epoch": 1.8466353677621283,
      "grad_norm": 2.1503055095672607,
      "learning_rate": 7.689793079464441e-06,
      "loss": 2.1864,
      "step": 17700
    },
    {
      "epoch": 1.8570683359415754,
      "grad_norm": 1.5372555255889893,
      "learning_rate": 7.620239958268127e-06,
      "loss": 2.2301,
      "step": 17800
    },
    {
      "epoch": 1.8675013041210224,
      "grad_norm": 1.4473447799682617,
      "learning_rate": 7.550686837071815e-06,
      "loss": 2.1345,
      "step": 17900
    },
    {
      "epoch": 1.8779342723004695,
      "grad_norm": 1.8419381380081177,
      "learning_rate": 7.481133715875501e-06,
      "loss": 2.1557,
      "step": 18000
    },
    {
      "epoch": 1.8883672404799166,
      "grad_norm": 1.507869005203247,
      "learning_rate": 7.411580594679187e-06,
      "loss": 2.1463,
      "step": 18100
    },
    {
      "epoch": 1.8988002086593636,
      "grad_norm": 2.044203281402588,
      "learning_rate": 7.342027473482873e-06,
      "loss": 2.1785,
      "step": 18200
    },
    {
      "epoch": 1.9092331768388107,
      "grad_norm": 1.6005234718322754,
      "learning_rate": 7.272474352286559e-06,
      "loss": 2.2146,
      "step": 18300
    },
    {
      "epoch": 1.9196661450182577,
      "grad_norm": 1.1840991973876953,
      "learning_rate": 7.202921231090245e-06,
      "loss": 2.1804,
      "step": 18400
    },
    {
      "epoch": 1.9300991131977048,
      "grad_norm": 1.6503971815109253,
      "learning_rate": 7.133368109893932e-06,
      "loss": 2.1439,
      "step": 18500
    },
    {
      "epoch": 1.9405320813771518,
      "grad_norm": 1.3059667348861694,
      "learning_rate": 7.063814988697618e-06,
      "loss": 2.1831,
      "step": 18600
    },
    {
      "epoch": 1.9509650495565989,
      "grad_norm": 1.5820485353469849,
      "learning_rate": 6.994261867501305e-06,
      "loss": 2.1198,
      "step": 18700
    },
    {
      "epoch": 1.961398017736046,
      "grad_norm": 1.3343636989593506,
      "learning_rate": 6.924708746304991e-06,
      "loss": 2.1836,
      "step": 18800
    },
    {
      "epoch": 1.971830985915493,
      "grad_norm": 1.6690006256103516,
      "learning_rate": 6.855155625108677e-06,
      "loss": 2.1643,
      "step": 18900
    },
    {
      "epoch": 1.98226395409494,
      "grad_norm": 1.9914079904556274,
      "learning_rate": 6.785602503912364e-06,
      "loss": 2.1884,
      "step": 19000
    },
    {
      "epoch": 1.992696922274387,
      "grad_norm": 1.9901976585388184,
      "learning_rate": 6.71604938271605e-06,
      "loss": 2.1729,
      "step": 19100
    },
    {
      "epoch": 2.0,
      "eval_loss": 2.409820318222046,
      "eval_runtime": 576.9552,
      "eval_samples_per_second": 20.851,
      "eval_steps_per_second": 2.607,
      "step": 19170
    }
  ],
  "logging_steps": 100,
  "max_steps": 28755,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.013062822483968e+16,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
